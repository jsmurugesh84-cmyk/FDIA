{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU9Z39gjky-V"
      },
      "outputs": [],
      "source": [
        "#Performance at different noise levels in SACGAN-GP for IEEE14-bus system and IEEE118-bus system\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load and analyze the combined dataset to identify preprocessing needs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate, Embedding\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Layer, Softmax\n",
        "\n",
        "# Load the combined dataset with attacked and non-attacked data\n",
        "revision_dataset_path = '/content/14bus_fdia_gaussian_noise_1pct.csv'\n",
        "df1 = pd.read_csv(revision_dataset_path)\n",
        "\n",
        "# Display basic information about the dataset to understand its structure\n",
        "df_info = df1.info()\n",
        "df_head = df1.head()\n",
        "\n",
        "df_info, df_head\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# -----------------------\n",
        "# 1. Extract Features & Labels\n",
        "# -----------------------\n",
        "feature_columns = [col for col in df1.columns if 'voltage' in col or 'angle' in col]\n",
        "X = df1[feature_columns]\n",
        "y_raw = df1['attack'] # Keep the raw float values if needed elsewhere\n",
        "\n",
        "# Binarize the 'attack' label for classification tasks\n",
        "# Assuming values > 0.5 are 'attacked' (1) and <= 0.5 are 'non-attacked' (0)\n",
        "y_binary = (y_raw > 0.5).astype(int)\n",
        "\n",
        "# -----------------------\n",
        "# 2. Normalize Features\n",
        "# -----------------------\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# ----------------------- FIRST SPLIT (for GAN-Attack data, not directly used in main GAN training) --------\n",
        "# Use y_binary and stratify to ensure class distribution in this split\n",
        "X_train_first_split, X_temp_test, y_train_first_split, y_temp_test = train_test_split(\n",
        "    X_normalized, y_binary, test_size=0.2, random_state=42, stratify=y_binary)\n",
        "\n",
        "# ----------------------- SECOND SPLIT (from Temp-Test => Final-Test + GAN-Attack) -------------------------\n",
        "# Stratify based on y_temp_test to ensure class distribution in these subsets\n",
        "X_test_final, X_gan_attack, y_test_final, y_gan_attack = train_test_split(\n",
        "    X_temp_test, y_temp_test, test_size=0.2, random_state=42, stratify=y_temp_test)\n",
        "\n",
        "# ----------------------- Convert to Tensors (for first split, if needed) -----------------------------------\n",
        "X_train_tensor_first_split = tf.convert_to_tensor(X_train_first_split, dtype=tf.float32)\n",
        "X_test_final_tensor = tf.convert_to_tensor(X_test_final, dtype=tf.float32)\n",
        "X_gan_attack_tensor = tf.convert_to_tensor(X_gan_attack, dtype=tf.float32)\n",
        "\n",
        "y_train_tensor_first_split = tf.convert_to_tensor(y_train_first_split.to_numpy(), dtype=tf.float32)\n",
        "y_test_final_tensor = tf.convert_to_tensor(y_test_final.to_numpy(), dtype=tf.float32)\n",
        "y_gan_attack_tensor = tf.convert_to_tensor(y_gan_attack.to_numpy(), dtype=tf.float32)\n",
        "\n",
        "# ----------------------- Print Shapes (for the first split) -----------------------------------------------\n",
        "print(\"TRAIN (first split for GAN-Attack generation):\", X_train_tensor_first_split.shape, y_train_tensor_first_split.shape)\n",
        "print(\"FINAL TEST (from second split):\", X_test_final_tensor.shape, y_test_final_tensor.shape)\n",
        "print(\"GAN ATTACK DATA (from second split):\", X_gan_attack_tensor.shape, y_gan_attack_tensor.shape)\n",
        "\n",
        "# === The actual split used for the GAN training/evaluation ===\n",
        "# This split will now correctly use the binarized labels and be stratified.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_binary, test_size=0.2, random_state=42, stratify=y_binary)\n",
        "\n",
        "print(\"\\n--- GAN Training/Evaluation Data Split ---\")\n",
        "print(\"X_train (for GAN training):\")\n",
        "print(X_train)\n",
        "print(\"X_test (for GAN evaluation):\")\n",
        "print(X_test)\n",
        "print(\"y_train (for GAN training):\")\n",
        "print(y_train)\n",
        "print(\"y_test (for GAN evaluation):\")\n",
        "print(y_test)\n",
        "\n",
        "# Convert NumPy arrays to Pandas DataFrames for easier saving\n",
        "X_train_df1 = pd.DataFrame(X_train)\n",
        "X_test_df1 = pd.DataFrame(X_test)\n",
        "y_train_df1 = pd.DataFrame(y_train) # y_train is now binary\n",
        "y_test_df1 = pd.DataFrame(y_test)   # y_test is now binary\n",
        "\n",
        "# Save to Excel files in the local /content/ directory\n",
        "X_train_df1.to_excel('/content/X_train5.xlsx', index=False)\n",
        "X_test_df1.to_excel('/content/X_test5.xlsx', index=False)\n",
        "y_train_df1.to_excel('/content/y_train5.xlsx', index=False)\n",
        "y_test_df1.to_excel('/content/y_test5.xlsx', index=False)\n",
        "\n",
        "# Convert data into TensorFlow tensors\n",
        "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "y_train_tensor = tf.convert_to_tensor(y_train.values, dtype=tf.float32) # y_train.values are 0s and 1s now\n",
        "y_test_tensor = tf.convert_to_tensor(y_test.values, dtype=tf.float32)   # y_test.values are 0s and 1s now\n",
        "\n",
        "print(\"\\nShapes of processed tensors (for GAN training/evaluation):\")\n",
        "print(\"X_train:\", X_train_tensor.shape)\n",
        "print(\"X_test:\", X_test_tensor.shape)\n",
        "print(\"y_train:\", y_train_tensor.shape)\n",
        "print(\"y_test:\", y_test_tensor.shape)\n",
        "\n",
        "\n",
        "class SelfAttention(Layer):\n",
        "    def __init__(self, units):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query = Dense(units // 8)\n",
        "        self.key = Dense(units // 8)\n",
        "        self.value = Dense(units)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query = self.query(inputs)\n",
        "        key = self.key(inputs)\n",
        "        value = self.value(inputs)\n",
        "\n",
        "        scores = tf.matmul(query, key, transpose_b=True)\n",
        "        attention_weights = Softmax()(scores)\n",
        "        attention_output = tf.matmul(attention_weights, value)\n",
        "        return attention_output + inputs  # Residual connection\n",
        "\n",
        "def build_cgan_generator(noise_dim, output_dim, num_classes=2):\n",
        "    noise_input = Input(shape=(noise_dim,))\n",
        "    label_input = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    label_embedding = Embedding(num_classes, noise_dim)(label_input)\n",
        "    label_embedding = Flatten()(label_embedding)\n",
        "\n",
        "    combined_input = Concatenate()([noise_input, label_embedding])\n",
        "\n",
        "    x = Dense(256, activation=\"relu\")(combined_input)\n",
        "    x = SelfAttention(256)(x)\n",
        "    x = Dense(output_dim, activation=\"tanh\")(x)\n",
        "    return Model([noise_input, label_input], x)\n",
        "\n",
        "def build_cgan_discriminator(input_dim, num_classes=2):\n",
        "    input_data = Input(shape=(input_dim,))\n",
        "    input_label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    label_embedding = Embedding(num_classes, input_dim)(input_label)\n",
        "    label_embedding = Flatten()(label_embedding)\n",
        "\n",
        "    combined_input = Concatenate()([input_data, label_embedding])\n",
        "\n",
        "    x = Dense(256, activation=\"relu\")(combined_input)\n",
        "    x = Dense(128, activation=\"relu\")(x) # Additional layer for the discriminator\n",
        "    output = Dense(1, activation=None)(x) # Output a raw score for WGAN-GP\n",
        "\n",
        "    return Model([input_data, input_label], output)\n",
        "\n",
        "def gradient_penalty(real, fake, label, discriminator):\n",
        "    alpha = tf.random.uniform([real.shape[0], 1], 0.0, 1.0)\n",
        "    interpolated = alpha * real + (1 - alpha) * fake\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated)\n",
        "        pred = discriminator([interpolated, label])\n",
        "\n",
        "    gradients = tape.gradient(pred, [interpolated])[0]\n",
        "    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=1))\n",
        "    return tf.reduce_mean((slopes - 1.0) ** 2)\n",
        "# Set up the GAN parameters\n",
        "noise_dim = 100\n",
        "output_dim = X_train.shape[1]\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "lambda_gp = 1 # Gradient penalty weight\n",
        "\n",
        "# Instantiate generator and discriminator models\n",
        "generator = build_cgan_generator(noise_dim, output_dim)\n",
        "discriminator = build_cgan_discriminator(output_dim)\n",
        "\n",
        "# Set optimizers\n",
        "generator_optimizer = Adam(1e-5)\n",
        "discriminator_optimizer = Adam(1e-5)\n",
        "\n",
        "# For tracking\n",
        "epoch_list = []\n",
        "d_loss_list = []\n",
        "g_loss_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Training loop\n",
        "# Convert labels to tensors in the training loop\n",
        "for epoch in range(epochs):\n",
        "    for _ in range(5):  # Update discriminator more frequently\n",
        "        # Sample random noise and labels\n",
        "        noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
        "        random_labels = np.random.randint(0, 2, batch_size)\n",
        "\n",
        "        # Convert noise and random labels to tensors\n",
        "        noise = tf.convert_to_tensor(noise, dtype=tf.float32)\n",
        "        random_labels = tf.convert_to_tensor(random_labels, dtype=tf.int32)\n",
        "\n",
        "        # Generate fake data conditioned on labels\n",
        "        generated_data = generator([noise, random_labels])\n",
        "\n",
        "        # Get a batch of real data and labels\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        real_data = X_train[idx]\n",
        "        real_labels = y_train.iloc[idx].values # y_train is now binary (0 or 1)\n",
        "\n",
        "        # Convert real labels to tensors\n",
        "        real_data = tf.convert_to_tensor(real_data, dtype=tf.float32)\n",
        "        real_labels = tf.convert_to_tensor(real_labels, dtype=tf.int32)\n",
        "\n",
        "        # Train Discriminator with gradient penalty\n",
        "        with tf.GradientTape() as tape:\n",
        "            d_loss_real = tf.reduce_mean(discriminator([real_data, real_labels]))\n",
        "            d_loss_fake = tf.reduce_mean(discriminator([generated_data, random_labels]))\n",
        "            gp = gradient_penalty(real_data, generated_data, real_labels, discriminator)\n",
        "            d_loss = d_loss_fake - d_loss_real + lambda_gp * gp\n",
        "\n",
        "        gradients = tape.gradient(d_loss, discriminator.trainable_variables)\n",
        "        discriminator_optimizer.apply_gradients(zip(gradients, discriminator.trainable_variables))\n",
        "\n",
        "    # Train Generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
        "    valid_labels = np.ones((batch_size,), dtype=int)  # Generate labels for attacked samples\n",
        "\n",
        "    # Convert noise and valid labels to tensors\n",
        "    noise = tf.convert_to_tensor(noise, dtype=tf.float32)\n",
        "    valid_labels = tf.convert_to_tensor(valid_labels, dtype=tf.int32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        g_loss = -tf.reduce_mean(discriminator([generator([noise, valid_labels]), valid_labels]))\n",
        "\n",
        "    gradients = tape.gradient(g_loss, generator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients, generator.trainable_variables))\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch}, D Loss: {d_loss.numpy():.4f}, G Loss: {g_loss.numpy():.4f}\")\n",
        "        # Generate fake samples for accuracy evaluation\n",
        "        generated_samples = generator([noise, valid_labels])\n",
        "        predictions = discriminator([generated_samples, valid_labels])\n",
        "\n",
        "        # Classify as attack if discriminator output < 0.5 (you can adjust threshold)\n",
        "        predicted_labels = tf.cast(predictions < 0.5, tf.int32)\n",
        "\n",
        "        # In a real case, compare against known attack labels (1 for valid_labels here)\n",
        "        acc = accuracy_score(valid_labels.numpy(), predicted_labels.numpy())\n",
        "\n",
        "        # Print and store\n",
        "        print(f\"Epoch: {epoch}, D Loss: {d_loss.numpy():.4f}, G Loss: {g_loss.numpy():.4f}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "        epoch_list.append(epoch)\n",
        "        d_loss_list.append(d_loss.numpy())\n",
        "        g_loss_list.append(g_loss.numpy())\n",
        "        accuracy_list.append(acc)\n",
        "\n",
        "\n",
        "# Generate anomaly scores for the test set\n",
        "discriminator_scores = discriminator.predict([X_test, y_test])\n",
        "\n",
        "# Choose a threshold (e.g., 75th percentile of scores for non-attacked samples)\n",
        "# y_test is now binary, so y_test == 0 will correctly select non-attacked samples\n",
        "threshold = np.percentile(discriminator_scores[y_test == 0], 75)\n",
        "predicted_labels = (discriminator_scores > threshold).astype(int)\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy = accuracy_score(y_test, predicted_labels)\n",
        "precision = precision_score(y_test, predicted_labels)\n",
        "recall = recall_score(y_test, predicted_labels)\n",
        "f1 = f1_score(y_test, predicted_labels)\n",
        "\n",
        "print(f\"Detection Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Detection Precision: {precision:.4f}\")\n",
        "print(f\"Detection Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# ===============================================\n",
        "# üîç FDIA Detection Evaluation Code\n",
        "# Confusion Matrix + ROC Curve + Precision‚ÄìRecall\n",
        "# ===============================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# -----------------------------------------------\n",
        "# 1Ô∏è‚É£  Get anomaly scores from the discriminator\n",
        "# -----------------------------------------------\n",
        "# Convert test labels to tensor\n",
        "y_test_tensor = tf.convert_to_tensor(y_test.values, dtype=tf.int32)\n",
        "X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "\n",
        "# Discriminator output score (higher => attack)\n",
        "scores = discriminator([X_test_tensor, y_test_tensor]).numpy().flatten()\n",
        "\n",
        "# Convert continuous scores to binary prediction using threshold\n",
        "threshold = np.percentile(scores, 75)      # ‚âà adaptive decision boundary\n",
        "y_pred = (scores > threshold).astype(int)\n",
        "\n",
        "# -----------------------------------------------\n",
        "# 3Ô∏è‚É£  ROC Curve + AUC\n",
        "# -----------------------------------------------\n",
        "fpr, tpr, _ = roc_curve(y_test, scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f\"AUC = {roc_auc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"ROC Curve - FDIA Detection\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# -----------------------------------------------\n",
        "# 4Ô∏è‚É£  Precision‚ÄìRecall Curve\n",
        "# -----------------------------------------------\n",
        "precision, recall, _ = precision_recall_curve(y_test, scores)\n",
        "avg_precision = average_precision_score(y_test, scores)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(recall, precision, linewidth=2, label=f\"AP = {avg_precision:.4f}\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision‚ÄìRecall Curve - FDIA Detection (Imbalanced Data)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAverage Precision Score: {avg_precision:.4f}\")\n"
      ]
    }
  ]
}